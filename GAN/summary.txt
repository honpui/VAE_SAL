1. It is observed that for AE, ELU is converging much faster than LeakyReLu using default setting. However, they eventually converge to similar value
2. Reconstruction quality gets much better when VAE loss decrease from 0.525 to 0.501. Thus, it is not a linear relation.
3. Initialization of AE is important to its training. Bad initial value can result in none converging. (squared error used)
4. cross entropy loss is much fast at converging than squared error
5. in training pure AE, the converging rate decreases fast with network goes deeper. X_rec_rec and x_rec are very similar in the begining in this case. Perhaps inception sturcture would be helpful
6. it is observed that the network first match the image in grey scale, the fine tune the details and the detailed color.
7. very large lr such as 0.1 is needed for adadelta in AE_celeba. But adaptive lr maybe desired later on

8. POSSIBLE ERROR HERE! For AE without FC layers (and some times with FC), rec_loss decrease with increasing number of layers, but rec_acc increase with increasing number of layers.
9. when FC is added, the reconstruction seems always getting "much worse". It is observed that the edges are lost, maybe due to the FC mixture.
10. When FC is added, more conv layers will improve the reconstruction accuracy, however, reconstruction accuracy is still much higher than AE without FC
11. Based on above finding, maybe FC can be removed in discriminator...

12. even larger lr as 0.5 is also fine for AE with 3 conv blocks and FC..
13. Keras VAE reconstruction result is much much better than the TF result

14. GAN with VAE with FC is converging asymptotically, however, the reconstruction is not very good due to original AE_FC
15. X_rec error in tensorboard and print is not the same. Base on the reconstructed image, there might be something wring in the tensorboard value.

16. A vanilla inception is converging super slow even for 2 blocked AE using DEFAULT LR. However, it can obtain better result than AE with 1/2/3 conventional VGG style blocks in the begining. But later on ~= 1 VGG block
17. Similar to VGG block, 3 Incep block is still worse than 2 Incep block

18. the value of k_t is important to training. Unconverging if set it to 1, converging well if set to 0.01

19. All generating cases have mode collapse so far. Only plausible generation is using parameter settings in GAN_2017_05_08_14_08_16. No k_t clip and x_input normalization here.
20. GAN_2017_05_08_15_15_09 tend to have balanced D and G loss

21. It is said in the repo issue that FC layer perfers no activation. This helps to solve mode collapse.

22. GAN_2017_05_10_09_19_02 in doi2 views G_gen and G_rec equally in GAN loss. Rec loss is trained on both enc and dec. Gen_loss is trained on dec.
23. GAN_2017_05_10_10_06_18 in doi5 put a weight 0.3 on G_rec in GAN loss. Rec loss is trained on both enc and dec. GAN loss is used to train dnc.

24. ALI sytle, fix rate of s_w to 0.5 will have gradient explosion for all s_
    ALI style embedding will result in mode collapse
    WGAN style embedding will result in gradient explosion of s_w
    WGAN style embedding with with k_t clipped in (0.1,1) will still have gradient explosion
    WGAN style embedding with with tanh output in AE won't have gradient explosion but has mode collapse again

25. IMPORTANT FIDING. Latent space with nonlinear activation will change the correlation of latent vectors! Thus, activation is not preferred at least in the AE FC layers.