
# class GAN(VAE):
#     def __init__(self,cfg):
#         super(GAN,self).__init__(cfg)
#
#         # Working and saving dir
#         self.logdir,self.modeldir = self.creat_dir('GAN')
#         save_config(cfg, self.modeldir)
#
#         # Network parameters for GAN
#         self.d_lr = cfg['d_lr']
#         self.g_lr = cfg['g_lr']
#         self.d_optimizer = tf.train.AdamOptimizer(self.d_lr)
#         self.g_optimizer = tf.train.AdamOptimizer(self.g_lr)
#
#         self.k_t = 1  # will be modified into PID controller in the future
#
#         if cfg['pre_train']:
#             self.train_vae(epochs=cfg['pre_train'])
#
#     def real_loss(self,x_input, x_rec):
#         # real_rec_loss = objectives.binary_crossentropy(x_input, x_gen)
#         # fake_rec_loss = objectives.binary_crossentropy(x_gen, x_gen_gen)
#         real_loss = tf.reduce_mean(tf.abs(x_input - x_rec))
#         return real_loss
#
#     def get_d_loss(self,real_loss, fake_loss):
#         d_loss = real_loss
#         for f_loss in fake_loss:
#             d_loss -= self.k_t * f_loss
#         return d_loss
#
#     def fake_loss(self,x_input, x_rec, x_rec_rec, x_gen, x_gen_rec):
#         rec_loss = tf.reduce_mean(tf.abs(x_rec - x_rec_rec))
#         gen_loss = tf.reduce_mean(tf.abs(x_gen - x_gen_rec))
#         return rec_loss, gen_loss
#     #     # x = self.norm_img(x_train)
#     #     # self.z = tf.random_uniform(
#     #     #     (tf.shape(x)[0], self.z_num), minval=-1.0, maxval=1.0)
#     #     # self.decoder(z)
#     #     # self.k_t = tf.Variable(0., trainable=False, name='k_t')
#     def train_gan(self, **kwargs):
#         '''model training'''
#         epochs = self.epochs
#         (self.X_train, self.y_train), (self.X_test, self.y_test) = self.CelebA(self.datadir)
#
#         with tf.Session() as sess:
#
#             with tf.variable_scope("vae_d", reuse=True):
#                 _, x_rec = self.def_vae(self.n_blocks, self.x_input)
#                 z_fixed = np.random.uniform(-1, 1, size=(self.batch_size, self.latent_dim)).astype('float32')
#                 x_gen = self.decoder(z_fixed,self.n_blocks)
#             with tf.variable_scope("vae_g", reuse=True):
#                 _, x_rec_rec = self.def_vae(self.n_blocks, x_rec)
#                 _, x_gen_rec = self.def_vae(self.n_blocks, x_gen)
#
#             real_loss = self.real_loss(self.x_input, x_rec)
#             fake_loss = self.fake_loss(self.x_input, x_rec, x_rec_rec, x_gen, x_gen_rec)
#             self.d_loss = self.get_d_loss(real_loss, fake_loss)
#             self.g_loss = fake_loss
#             self.log_vars.append(("d_loss", tf.reduce_mean(self.d_loss)))
#             d_trainer = self.d_optimizer.minimize(self.d_loss)  # , var_list=self.vae_var
#             all_g_trainer = []
#             for g_loss in self.g_loss:
#                 all_g_trainer.append(self.g_optimizer.minimize(g_loss))  # , var_list=self.vae_var
#                 self.log_vars.append(("g_loss", tf.reduce_mean(g_loss)))
#             rec_er = tf.reduce_mean(tf.sqrt(tf.squared_difference(self.x_input, x_rec)))
#             self.log_vars.append(("rec_er", tf.reduce_mean(rec_er)))
#
#             init = tf.global_variables_initializer()
#             sess.run(init)
#             log_vars = [x for _, x in self.log_vars]
#             log_keys = [x for x, _ in self.log_vars]
#             for k, v in self.log_vars:
#                 tf.summary.scalar(k, v)
#             summary_op = tf.summary.merge_all()
#             summary_writer = tf.summary.FileWriter(self.logdir, sess.graph)
#             saver = tf.train.Saver()
#
#             counter = 0
#             for epoch in tqdm(range(epochs)):
#                 it_per_ep = len(self.X_train) / self.batch_size
#                 all_log_vals = []
#                 for i in range(it_per_ep):  # 1875 * 32 = 60000 -> # of training samples
#                     counter += 1
#                     x_train = self.X_train[i * self.batch_size:(i + 1) * self.batch_size]
#                     y_train = self.y_train[i * self.batch_size:(i + 1) * self.batch_size]
#                     feed_dict = {self.x_input: x_train, self.y_input: y_train}
#                     # with tf.control_dependencies([d_optim, g_optim]):
#                     #     self.k_update = tf.assign(
#                     #         self.k_t, tf.clip_by_value(self.k_t + self.lambda_k * self.balance, 0, 1))
#                     log_vals = sess.run([d_trainer] + log_vars, feed_dict)[1:]
#                     for g_trainer in all_g_trainer:
#                         log_vals = sess.run([g_trainer] + log_vars, feed_dict)[1:]
#                     all_log_vals.append(log_vals)
#
#                     if counter % self.snapshot_interval == 0:
#                         snapshot_name = "%s_%s" % ('experiment', str(counter))
#                         fn = saver.save(sess, "%s/%s.ckpt" % (self.modeldir, snapshot_name))
#                         print("Model saved in file: %s" % fn)
#                 # output to terminal
#                 avg_log_vals = np.mean(np.array(all_log_vals), axis=0)
#                 log_line = "; ".join("%s: %s" % (str(k), str(v)) for k, v in zip(log_keys, avg_log_vals))
#                 print("Epoch %d | " % (epoch) + log_line)
#                 sys.stdout.flush()
#                 if np.any(np.isnan(avg_log_vals)):
#                     raise ValueError("NaN detected!")
#                 # output to tensorboard
#                 x_train = self.X_train[i * self.batch_size:(i + 1) * self.batch_size]
#                 y_train = self.y_train[i * self.batch_size:(i + 1) * self.batch_size]
#                 feed_dict = {self.x_input: x_train, self.y_input: y_train}
#                 summary_str = sess.run(summary_op, feed_dict)
#                 summary_writer.add_summary(summary_str, counter)
#                 # save reconstructed images
#                 x_rec_img = sess.run(x_rec, feed_dict)
#                 x_rec_rec_img = sess.run(x_rec_rec, feed_dict)
#                 all_G_z = np.concatenate([255 * x_train[0:8], 255 * x_rec_img[0:8], 255 * x_rec_rec_img[0:8]])
#                 save_image(all_G_z, '{}/epoch_{}_{}.png'.format(self.logdir, epoch, log_line))


#
# class GAN2(object):
#     def __init__(self,cfg):
#         self.log_vars = []
#
#         self.vae_g = VAE(cfg)
#         self.vae_d = VAE(cfg)
#         # Working and saving dir
#         self.logdir,self.modeldir = self.vae_g.creat_dir('GAN')
#         save_config(cfg, self.modeldir)
#
#         # Network parameters for GAN
#         self.epochs = cfg['max_epochs']
#         self.d_lr = cfg['d_lr']
#         self.g_lr = cfg['g_lr']
#         self.d_optimizer = tf.train.AdamOptimizer(self.d_lr)
#         self.g_optimizer = tf.train.AdamOptimizer(self.g_lr)
#
#         self.k_t = 1  # will be modified into PID controller in the future
#
#         if cfg['pre_train']:
#             self.vae_g.train_vae(epochs=cfg['pre_train'])
#             self.vae_d.train_vae(epochs=cfg['pre_train'])
#
#         # # Network Parameter Setting
#         # self.variational = cfg['vae']
#         # self.act_func = self.activation(cfg)
#         self.latent_dim = cfg['latent_dim']
#         # self.n_filter = cfg['n_filters']
#         # self.filter_size = cfg['filter_size']
#         self.img_size = cfg['input_dim'][0]
#         self.n_ch_in = cfg['n_channels']
#         self.n_attributes = cfg['n_attributes']
#         # self.datadir = cfg['datadir']
#         # self.encoder_module = self.VGG_enc_block
#         # self.decoder_module = self.VGG_dec_block
#         self.n_blocks = cfg['n_blocks']
#         # define VAE network
#         # Optimizer Parameter Setting
#         self.batch_size = cfg['batch_size']
#         self.optimizer = cfg['vae_optimizer']
#         self.vae_lr = cfg['vae_lr']
#         self.snapshot_interval = cfg['snapshot_interval']
#
#         self.x_input = tf.placeholder(tf.float32, [self.batch_size, self.img_size, self.img_size, self.n_ch_in])
#         self.y_input = tf.placeholder(tf.float32, [self.batch_size, self.n_attributes])
#
#     def real_loss(self,x_input, x_rec):
#         # real_rec_loss = objectives.binary_crossentropy(x_input, x_gen)
#         # fake_rec_loss = objectives.binary_crossentropy(x_gen, x_gen_gen)
#         real_loss = tf.reduce_mean(tf.abs(x_input - x_rec))
#         return real_loss
#
#     def get_d_loss(self,real_loss, fake_loss):
#         d_loss = real_loss
#         for f_loss in fake_loss:
#             d_loss -= self.k_t * f_loss
#         return d_loss
#
#     def fake_loss(self,x_input, x_rec, x_rec_rec, x_gen, x_gen_rec):
#         rec_loss = tf.reduce_mean(tf.abs(x_rec - x_rec_rec))
#         gen_loss = tf.reduce_mean(tf.abs(x_gen - x_gen_rec))
#         return rec_loss, gen_loss
#     #     # x = self.norm_img(x_train)
#     #     # self.z = tf.random_uniform(
#     #     #     (tf.shape(x)[0], self.z_num), minval=-1.0, maxval=1.0)
#     #     # self.decoder(z)
#     #     # self.k_t = tf.Variable(0., trainable=False, name='k_t')
#     def train_gan(self, **kwargs):
#         '''model training'''
#         epochs = self.epochs
#         (self.X_train, self.y_train), (self.X_test, self.y_test) = self.vae_g.CelebA(self.vae_g.datadir)
#
#         with tf.Session() as sess:
#
#             with tf.variable_scope("vae_g", reuse=True):
#                 _, x_rec = self.vae_g.def_vae(self.n_blocks, self.x_input)
#                 z_fixed = np.random.uniform(-1, 1, size=(self.batch_size, self.latent_dim)).astype('float32')
#                 x_gen = self.vae_g.decoder(z_fixed,self.n_blocks)
#             with tf.variable_scope("vae_d", reuse=True):
#                 _, x_rec_rec = self.vae_d.def_vae(self.n_blocks, x_rec)
#                 _, x_gen_rec = self.vae_d.def_vae(self.n_blocks, x_gen)
#
#             real_loss = self.real_loss(self.x_input, x_rec)
#             self.log_vars.append(("real_loss", tf.reduce_mean(real_loss)))
#             fake_loss = self.fake_loss(self.x_input, x_rec, x_rec_rec, x_gen, x_gen_rec)
#             self.d_loss = self.get_d_loss(real_loss, fake_loss)
#             self.g_loss = fake_loss
#
#             all_vars = tf.trainable_variables()
#             d_vars = [var for var in all_vars if var.name.startswith('vae_d')]
#             g_vars = [var for var in all_vars if var.name.startswith('vae_g')]
#             d_trainer = self.d_optimizer.minimize(self.d_loss, var_list=d_vars)  # , var_list=self.vae_var
#             self.log_vars.append(("d_loss", tf.reduce_mean(self.d_loss)))
#             all_g_trainer = []
#             for i,g_loss in enumerate(self.g_loss):
#                 all_g_trainer.append(self.g_optimizer.minimize(g_loss, var_list=g_vars))  # , var_list=self.vae_var
#                 self.log_vars.append(("g_loss_{}".format(i), tf.reduce_mean(g_loss)))
#             rec_er = tf.reduce_mean(tf.sqrt(tf.squared_difference(self.x_input, x_rec)))
#             self.log_vars.append(("rec_er", tf.reduce_mean(rec_er)))
#
#             init = tf.global_variables_initializer()
#             sess.run(init)
#             log_vars = [x for _, x in self.log_vars]
#             log_keys = [x for x, _ in self.log_vars]
#             for k, v in self.log_vars:
#                 tf.summary.scalar(k, v)
#             summary_op = tf.summary.merge_all()
#             summary_writer = tf.summary.FileWriter(self.logdir, sess.graph)
#             saver = tf.train.Saver()
#
#             counter = 0
#             for epoch in tqdm(range(epochs)):
#                 it_per_ep = len(self.X_train) / self.batch_size
#                 all_log_vals = []
#                 for i in range(it_per_ep):  # 1875 * 32 = 60000 -> # of training samples
#                     counter += 1
#                     x_train = self.X_train[i * self.batch_size:(i + 1) * self.batch_size]
#                     y_train = self.y_train[i * self.batch_size:(i + 1) * self.batch_size]
#                     feed_dict = {self.x_input: x_train, self.y_input: y_train}
#                     # with tf.control_dependencies([d_optim, g_optim]):
#                     #     self.k_update = tf.assign(
#                     #         self.k_t, tf.clip_by_value(self.k_t + self.lambda_k * self.balance, 0, 1))
#                     log_vals = sess.run([d_trainer] + log_vars, feed_dict)[1:]
#                     for g_trainer in all_g_trainer:
#                         log_vals = sess.run([g_trainer] + log_vars, feed_dict)[1:]
#                     all_log_vals.append(log_vals)
#
#                     if counter % self.snapshot_interval == 0:
#                         snapshot_name = "%s_%s" % ('experiment', str(counter))
#                         fn = saver.save(sess, "%s/%s.ckpt" % (self.modeldir, snapshot_name))
#                         print("Model saved in file: %s" % fn)
#                 # output to terminal
#                 avg_log_vals = np.mean(np.array(all_log_vals), axis=0)
#                 log_line = "; ".join("%s: %s" % (str(k), str(v)) for k, v in zip(log_keys, avg_log_vals))
#                 print("Epoch %d | " % (epoch) + log_line)
#                 sys.stdout.flush()
#                 if np.any(np.isnan(avg_log_vals)):
#                     raise ValueError("NaN detected!")
#                 # output to tensorboard
#                 x_train = self.X_train[i * self.batch_size:(i + 1) * self.batch_size]
#                 y_train = self.y_train[i * self.batch_size:(i + 1) * self.batch_size]
#                 feed_dict = {self.x_input: x_train, self.y_input: y_train}
#                 summary_str = sess.run(summary_op, feed_dict)
#                 summary_writer.add_summary(summary_str, counter)
#                 # save reconstructed images
#                 x_rec_img = sess.run(x_rec, feed_dict)
#                 x_rec_rec_img = sess.run(x_rec_rec, feed_dict)
#                 all_G_z = np.concatenate([255 * x_train[0:8], 255 * x_rec_img[0:8], 255 * x_rec_rec_img[0:8]])
#                 save_image(all_G_z, '{}/epoch_{}_{}.png'.format(self.logdir, epoch, log_line))
#



#
#
# class GAN3(object):
#     def __init__(self,cfg):
#         self.log_vars = []
#         self.cfg = cfg
#
#         self.gamma = tf.cast(cfg['gamma'], tf.float32)
#         self.lambda_k = tf.cast(cfg['lambda_k'], tf.float32)
#         self.k_t = cfg['k_t']
#         self.log_vars.append(("k_t", tf.reduce_mean(self.k_t)))
#
#         cfg_g = cfg
#         # cfg_g['batch_size'] = cfg['batch_size']/2
#         self.vae_g = VAE(cfg_g)
#         cfg_d = cfg
#         cfg_d['batch_size'] = cfg['batch_size']*2
#         self.vae_d = VAE(cfg_d)
#         # Working and saving dir
#         self.logdir,self.modeldir = self.vae_g.creat_dir('GAN')
#         save_config(cfg, self.modeldir)
#
#         # Network parameters for GAN
#         self.epochs = cfg['max_epochs']
#         self.d_lr = cfg['d_lr']
#         self.g_lr = cfg['g_lr']
#         self.g_optimizer = tf.train.AdamOptimizer(self.g_lr)
#         if cfg['d_optimizer'] == 'adam':
#             self.d_optimizer = tf.train.AdamOptimizer(self.d_lr)
#         elif cfg['d_optimizer'] == 'adadelta':
#             self.d_optimizer = tf.train.AdadeltaOptimizer(self.d_lr)
#         elif cfg['d_optimizer'] == 'adagrad':
#             self.d_optimizer = tf.train.AdagradOptimizer(self.d_lr)
#         else:
#             raise Exception("[!] Caution! {} opimizer is not implemented in VAE training".format(self.optimizer))
#
#
#         if cfg['pre_train']:
#             self.vae_g.train_vae(epochs=cfg['pre_train'])
#             self.vae_d.train_vae(epochs=cfg['pre_train'])
#
#         # # Network Parameter Setting
#         # self.variational = cfg['vae']
#         # self.act_func = self.activation(cfg)
#         self.latent_dim = cfg['latent_dim']
#         # self.n_filter = cfg['n_filters']
#         # self.filter_size = cfg['filter_size']
#         self.img_size = cfg['input_dim'][0]
#         self.n_ch_in = cfg['n_channels']
#         self.n_attributes = cfg['n_attributes']
#         # self.datadir = cfg['datadir']
#         # self.encoder_module = self.VGG_enc_block
#         # self.decoder_module = self.VGG_dec_block
#         self.n_blocks = cfg['n_blocks']
#         # define VAE network
#         # Optimizer Parameter Setting
#         # self.batch_size = cfg['batch_size']
#         self.optimizer = cfg['vae_optimizer']
#         self.vae_lr = cfg['vae_lr']
#         self.snapshot_interval = cfg['snapshot_interval']
#
#         self.x_input = tf.placeholder(tf.float32, [self.vae_g.batch_size, self.img_size, self.img_size, self.n_ch_in],name='x_input')
#         self.y_input = tf.placeholder(tf.float32, [self.vae_g.batch_size, self.n_attributes],name='y_input')
#         self.z_input = tf.placeholder(tf.float32, [self.vae_g.batch_size, self.latent_dim],name='z_input')
#
#     def get_real_loss(self,x_input, x_rec):
#         # real_rec_loss = objectives.binary_crossentropy(x_input, x_gen)
#         # fake_rec_loss = objectives.binary_crossentropy(x_gen, x_gen_gen)
#         real_loss = tf.reduce_mean(tf.abs(x_input - x_rec))
#         return real_loss
#
#     def get_d_loss(self,real_loss, fake_loss):
#         d_loss = real_loss
#         # for f_loss in fake_loss:
#         d_loss -= self.k_t * fake_loss
#         return d_loss
#
#     def get_fake_loss(self,x_gen, x_gen_rec):
#         gen_loss = tf.reduce_mean(tf.abs(x_gen - x_gen_rec))
#         return gen_loss
#
#     def train_gan(self, **kwargs):
#         '''model training'''
#         epochs = self.epochs
#         (self.X_train, self.y_train), (self.X_test, self.y_test) = self.vae_g.CelebA(self.vae_g.datadir)
#
#         gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)
#         with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
#
#             # get variable
#             with tf.variable_scope("vae_g", reuse=True):
#                 # _, x_rec = self.vae_g.def_vae(self.n_blocks, self.x_input)
#                 x_gen = self.vae_g.decoder(self.z_input,self.n_blocks)
#                 x_gen = tf.identity(x_gen, name="x_gen")
#                 sig_x_gen = tf.nn.sigmoid(x_gen, name='sig_x_gen')
#             with tf.variable_scope("vae_d", reuse=True):
#                 # # # with tf.variable_scope("pass1", reuse=True):
#                 # x_rec , sig_x_rec = self.vae_d.def_vae(self.n_blocks, self.x_input)
#                 # x_rec = tf.identity(x_rec, name="x_rec")
#                 # # with tf.variable_scope("pass2", reuse=True):
#                 # x_gen_rec, sig_x_gen_rec = self.vae_d.def_vae(self.n_blocks, x_gen)
#                 # x_gen_rec = tf.identity(x_gen_rec, name="x_gen_rec")
#                 # _, x_rec_rec = self.vae_d.def_vae(self.n_blocks, x_rec)
#                 d_out, _ = self.vae_d.def_vae(self.n_blocks, tf.concat([self.x_input, x_gen], 0))
#                 x_rec, x_gen_rec = tf.split(d_out,2)
#
#             # Get loss_gen
#             self.real_loss = self.get_real_loss(self.x_input, x_rec)
#             self.log_vars.append(("real_loss", tf.reduce_mean(self.real_loss)))
#             self.g_loss = self.get_fake_loss(x_gen, x_gen_rec)
#             self.log_vars.append(("fake_loss", tf.reduce_mean(self.g_loss)))
#             self.d_loss = self.get_d_loss(self.real_loss, self.g_loss)
#             self.log_vars.append(("d_loss", tf.reduce_mean(self.d_loss)))
#
#             self.balance = self.gamma * self.real_loss - self.g_loss
#             self.log_vars.append(("balance", tf.reduce_mean(self.balance)))
#             self.measure = self.real_loss + tf.abs(self.balance)
#             self.log_vars.append(("measure", tf.reduce_mean(self.measure)))
#             self.k_t_update = self.lambda_k * self.balance
#             self.log_vars.append(("k_t_update", tf.reduce_mean(self.k_t_update)))
#
#             all_vars = tf.trainable_variables()
#             d_vars = [var for var in all_vars if var.name.startswith('vae_d')]
#             g_vars = [var for var in all_vars if var.name.startswith('vae_g')]
#             d_trainer = self.d_optimizer.minimize(self.d_loss, var_list=d_vars)
#             g_trainer = self.g_optimizer.minimize(self.g_loss, var_list=g_vars)
#
#             # run iteration
#             init = tf.global_variables_initializer()
#             sess.run(init)
#             log_vars = [x for _, x in self.log_vars]
#             log_keys = [x for x, _ in self.log_vars]
#             for k, v in self.log_vars:
#                 tf.summary.scalar(k, v)
#             summary_op = tf.summary.merge_all()
#             summary_writer = tf.summary.FileWriter(self.logdir, sess.graph)
#             saver = tf.train.Saver()
#
#             counter = 0
#             for epoch in tqdm(range(epochs)):
#                 it_per_ep = len(self.X_train) / self.vae_g.batch_size
#                 all_log_vals = []
#                 for i in range(it_per_ep):  # 1875 * 32 = 60000 -> # of training samples
#                     counter += 1
#                     x_train = self.X_train[i * self.vae_g.batch_size:(i + 1) * self.vae_g.batch_size]
#                     y_train = self.y_train[i * self.vae_g.batch_size:(i + 1) * self.vae_g.batch_size]
#                     z_input = np.random.uniform(-1, 1, size=(self.vae_g.batch_size, self.latent_dim)).astype('float32')
#                     feed_dict = {self.x_input: x_train, self.y_input: y_train, self.z_input:z_input}
#                     # train D
#                     sess.run(d_trainer, feed_dict)
#                     # train G
#                     log_vals = sess.run([g_trainer]+log_vars, feed_dict)[1:]
#                     # train k_t and get logs
#                     self.k_t += sess.run(self.k_t_update,feed_dict)
#                     if self.k_t<0.0:
#                         self.k_t = 0.0
#                     elif self.k_t>1.0:
#                         self.k_t = 1.0
#                     log_vals[0] = self.k_t
#                     all_log_vals.append(log_vals)
#
#                     if counter % self.snapshot_interval == 0:
#                         snapshot_name = "%s_%s" % ('experiment', str(counter))
#                         fn = saver.save(sess, "%s/%s.ckpt" % (self.modeldir, snapshot_name))
#                         print("Model saved in file: %s" % fn)
#
#                 # output to terminal
#                 avg_log_vals = np.mean(np.array(all_log_vals), axis=0)
#                 log_line = "; ".join("%s: %s" % (str(k), str(v)) for k, v in zip(log_keys, avg_log_vals))
#                 print("Epoch %d | " % (epoch) + log_line)
#                 sys.stdout.flush()
#                 if np.any(np.isnan(avg_log_vals)):
#                     raise ValueError("NaN detected!")
#
#                 # output to tensorboard
#                 x_train = self.X_train[i * self.vae_g.batch_size:(i + 1) * self.vae_g.batch_size]
#                 y_train = self.y_train[i * self.vae_g.batch_size:(i + 1) * self.vae_g.batch_size]
#                 z_input = np.random.uniform(-1, 1, size=(self.vae_g.batch_size, self.latent_dim)).astype('float32')
#                 feed_dict = {self.x_input: x_train, self.y_input: y_train, self.z_input:z_input}
#                 summary_str = sess.run(summary_op, feed_dict)
#                 summary_writer.add_summary(summary_str, counter)
#
#                 # save reconstructed images
#                 x_rec_img = sess.run(x_rec, feed_dict)
#                 x_gen_img = sess.run(x_gen, feed_dict)
#                 # feed_dict = {self.x_input: x_gen_img, self.y_input: y_train, self.z_input:z_input}
#                 # x_gen_rec_img = sess.run(x_rec, feed_dict)
#                 x_gen_rec_img = sess.run(x_gen_rec, feed_dict)
#                 all_G_z = np.concatenate([255 * (x_train[0:8] + 1) / 2, 255 * (x_rec_img[0:8] + 1) / 2,
#                                           255 * (x_gen_img[0:8] + 1) / 2,255 * (x_gen_rec_img[0:8] + 1) / 2])
#                 save_image(all_G_z, '{}/epoch_{}_{}.png'.format(self.logdir, epoch, log_line))
